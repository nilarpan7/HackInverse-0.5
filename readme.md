# COSMEON Orbital Data Layer ğŸŒŒ

**A Fault-Tolerant Distributed Storage System with Intelligent Erasure Coding**

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-green.svg)](https://fastapi.tiangolo.com/)
[![Supabase](https://img.shields.io/badge/Supabase-Storage-purple.svg)](https://supabase.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ¯ Overview

COSMEON is a distributed storage system that splits files into encoded shards and distributes them across multiple storage nodes. Even if some nodes fail, the original file can be completely reconstructed from the remaining shards. This system implements **erasure coding** with intelligent algorithm selection to balance storage efficiency, performance, and cost.

## âœ¨ Features

- **ğŸ§© Intelligent Erasure Coding**: Automatically selects between Reed-Solomon, XOR-Parity, or Replication based on file characteristics
- **ğŸ›¡ï¸ Fault Tolerance**: Files can survive node failures (configurable from 1 to n nodes)
- **ğŸ“Š Smart Analytics**: Analyzes files to choose optimal storage strategy
- **ğŸŒ Distributed Storage**: Uses Supabase Storage as distributed nodes
- **ğŸš€ FastAPI Backend**: RESTful API for all operations
- **ğŸ–¥ï¸ Rich CLI**: Beautiful terminal interface with progress bars and color-coded status
- **ğŸ“ˆ Cost Optimization**: Estimates storage costs and suggests optimizations

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    CLI Client   â”‚â”€â”€â”€â”€â–¶â”‚  FastAPI Engine â”‚â”€â”€â”€â”€â–¶â”‚  Storage Grid   â”‚
â”‚  (Typer/Rich)   â”‚â—€â”€â”€â”€â”€â”‚  (Python)       â”‚â—€â”€â”€â”€â”€â”‚  (5 Supabase    â”‚
â”‚                 â”‚     â”‚                 â”‚     â”‚    Buckets)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                         â”‚                         â”‚
       â”‚                         â”‚                         â”‚
       â–¼                         â–¼                         â–¼
 User Commands          Algorithm Selection          Shard Distribution
   â€¢ upload             â€¢ File Analysis              â€¢ node-1
   â€¢ reconstruct        â€¢ Reed-Solomon               â€¢ node-2  
   â€¢ status             â€¢ XOR-Parity                 â€¢ node-3
   â€¢ simulate-failure   â€¢ Replication                â€¢ node-4
                                                 â€¢ node-5
```

## ğŸ“¦ Storage Nodes Configuration

The system uses **5 Supabase Storage buckets** to simulate a distributed storage grid:

| Node | Bucket Name | Role | Purpose |
|------|-------------|------|---------|
| ğŸŸ¢ Node 1 | `node-1` | Primary | Stores data shards |
| ğŸŸ¢ Node 2 | `node-2` | Primary | Stores data shards |
| ğŸŸ¢ Node 3 | `node-3` | Primary | Stores data shards |
| ğŸŸ¡ Node 4 | `node-4` | Parity | Stores parity/backup shards |
| ğŸŸ¡ Node 5 | `node-5` | Parity | Stores parity/backup shards |

**Default Encoding**: Reed-Solomon (3,2) scheme:
- **3 data shards** (minimum needed for reconstruction)
- **2 parity shards** (can lose any 2 nodes)
- **Storage overhead**: 167% (5/3 = 1.67x original size)

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- [Supabase](https://supabase.com) account (free tier)
- Git

### 1. Clone the Repository
```bash
git clone https://github.com/yourusername/cosmeon.git
cd cosmeon
```

### 2. Set Up Supabase
1. Create a new project at [supabase.com](https://supabase.com)
2. Create 5 storage buckets: `node-1`, `node-2`, `node-3`, `node-4`, `node-5`
3. Make all buckets **public** (for demo purposes)
4. Copy your project credentials

### 3. Configure Environment
```bash
# Copy the example env file
cp .env.example .env

# Edit .env with your Supabase credentials
nano .env
```

Your `.env` should contain:
```env
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your-anon-key
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key

# Storage configuration
STORAGE_BUCKETS=node-1,node-2,node-3,node-4,node-5
DEFAULT_ALGORITHM=reed-solomon
```

### 4. Install Dependencies
```bash
pip install -r requirements.txt
```

### 5. Initialize Storage
```bash
python setup_supabase.py
```

### 6. Start the Server
```bash
uvicorn main:app --reload
```
The API will be available at `http://localhost:8000`

### 7. Use the CLI
```bash
# Upload a file
cosmeon upload sample.jpg

# Upload with specific algorithm
cosmeon upload backup.zip --algorithm=reed-solomon

# Check file status
cosmeon status <file_id>

# Reconstruct a file
cosmeon reconstruct <file_id> --output=recovered.jpg

# Simulate node failure (demo)
cosmeon simulate-failure node-2
```

## ğŸ“– API Reference

### Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Health check and system info |
| `GET` | `/nodes/status` | Check status of all storage nodes |
| `POST` | `/upload` | Upload a file with smart encoding |
| `GET` | `/file/{file_id}/status` | Check reconstruction status of a file |
| `GET` | `/file/{file_id}/reconstruct` | Reconstruct file from shards |
| `GET` | `/analytics` | System analytics and cost breakdown |

### Upload a File
```bash
curl -X POST \
  -F "file=@document.pdf" \
  -F "algorithm=reed-solomon" \
  http://localhost:8000/upload
```

Response:
```json
{
  "file_id": "550e8400-e29b-41d4-a716-446655440000",
  "algorithm": "reed-solomon",
  "shards": [
    {"bucket": "node-1", "url": "...", "size": 10240},
    {"bucket": "node-2", "url": "...", "size": 10240},
    {"bucket": "node-3", "url": "...", "size": 10240},
    {"bucket": "node-4", "url": "...", "size": 10240},
    {"bucket": "node-5", "url": "...", "size": 10240}
  ],
  "storage_cost": 0.00045,
  "can_survive_failures": 2
}
```

## ğŸ§  Smart Algorithm Selection

COSMEON analyzes each file to choose the optimal storage strategy:

### Algorithms Available

| Algorithm | Use Case | Overhead | Fault Tolerance | Best For |
|-----------|----------|----------|-----------------|----------|
| **Replication** | Hot data, small files | 300% (3x) | 2/5 nodes | Logs, databases, temp files |
| **Reed-Solomon** | Cold data, large files | 167% (1.67x) | ANY 2 nodes | Backups, archives, videos |
| **XOR-Parity** | Balanced needs | 200% (2x) | SOME 2 nodes | Images, documents, mixed use |

### Decision Factors
- **File size**: Small â†’ Replication, Large â†’ Reed-Solomon
- **File type**: Media â†’ XOR-Parity, Archives â†’ Reed-Solomon
- **Access pattern**: Hot â†’ Replication, Cold â†’ Reed-Solomon
- **Criticality**: High â†’ Replication, Low â†’ Reed-Solomon

## ğŸ”§ Advanced Usage

### Custom Algorithm Parameters
```bash
# Reed-Solomon with custom parameters (k=4, m=3)
cosmeon upload data.bin --algorithm=reed-solomon --k=4 --m=3

# Replication with 5 copies
cosmeon upload critical.log --algorithm=replication --factor=5

# XOR-Parity with 4 data + 3 parity shards
cosmeon upload dataset.csv --algorithm=xor-parity --data=4 --parity=3
```

### Policy-Based Upload
```bash
# Use cost-optimization policy
cosmeon upload archive.tar.gz --policy=cost

# Use performance-optimization policy  
cosmeon upload video.mp4 --policy=performance

# Use balanced policy (default)
cosmeon upload document.pdf --policy=balanced
```

### System Analytics
```bash
# Get storage analytics
cosmeon analytics

# Check individual file efficiency
cosmeon analyze <file_id>
```

## ğŸ§ª Demonstration: Fault Tolerance

1. **Upload a file:**
```bash
cosmeon upload important_document.pdf
```

2. **Check where shards are stored:**
```bash
cosmeon status <file_id>
```
Output shows shards distributed across 5 nodes.

3. **Simulate node failures:**
```bash
# Simulate node-2 going offline
cosmeon simulate-failure node-2

# Simulate another failure
cosmeon simulate-failure node-5
```

4. **Reconstruct the file:**
```bash
# Even with 2 nodes down, we can reconstruct!
cosmeon reconstruct <file_id> --output=recovered.pdf
```

The system successfully reconstructs the file using the remaining 3 shards!

## ğŸ“Š Performance Characteristics

| Operation | Replication | Reed-Solomon | XOR-Parity |
|-----------|-------------|--------------|------------|
| **Upload Speed** | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡ |
| **Download Speed** | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡ | âš¡âš¡âš¡âš¡ |
| **Reconstruction** | âš¡âš¡âš¡âš¡âš¡ | âš¡ | âš¡âš¡âš¡ |
| **Storage Efficiency** | âš¡ | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡ |
| **Fault Tolerance** | âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡ |

## ğŸ—‚ï¸ Project Structure

```
cosmeon/
â”œâ”€â”€ main.py                 # FastAPI server
â”œâ”€â”€ cli.py                  # Command-line interface
â”œâ”€â”€ storage_manager.py      # Supabase storage operations
â”œâ”€â”€ smart_engine.py         # Intelligent algorithm selection
â”œâ”€â”€ algorithms.py           # Encoding/decoding implementations
â”œâ”€â”€ setup_supabase.py       # Initialization script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ .env.example           # Environment template
â”œâ”€â”€ config.yaml            # System configuration
â””â”€â”€ README.md              # This file
```

## ğŸ” Security Considerations

> **Note**: This is a demonstration system. For production use:

1. **Don't use public buckets** - Implement signed URLs
2. **Add authentication** - Use Supabase Auth or JWT
3. **Encrypt sensitive data** - Encrypt before encoding
4. **Implement rate limiting** - Prevent abuse
5. **Use private endpoints** - Don't expose admin functions

## ğŸš§ Limitations

- **File Size**: Supabase free tier limits files to 50MB
- **Total Storage**: 5GB limit on free tier
- **Public Access**: Buckets are public for demo purposes
- **Node Count**: Fixed at 5 nodes in this implementation

## ğŸ“ˆ Scaling Beyond 5 Nodes

For production with more nodes, see our [Scaling Guide](docs/SCALING.md) which covers:
- Dynamic node allocation (10-100+ nodes)
- Multiple storage providers
- Geographic distribution
- Cost-tiered storage
- Automatic rebalancing

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Reed-Solomon implementation: `reedsolo` library
- Storage: Supabase
- CLI: Typer + Rich
- API Framework: FastAPI
- Inspired by: RAID systems, Erasure Coding in distributed storage
